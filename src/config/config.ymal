experiment:
  name: "gahpc_experiment"
  seed: 42

data:
  job_trace_path: "data/job_logs/"
  batch_size: 64
  sequence_length: 128

gan:
  model: "wgan-gp"
  latent_dim: 32
  generator_dim: 128
  critic_dim: 128
  lambda_gp: 10
  lr: 1e-4
  epochs: 50

timegan:
  hidden_dim: 48
  num_layers: 3
  learning_rate: 1e-3
  training_steps: 20000

ppo:
  learning_rate: 3e-4
  gamma: 0.99
  lam: 0.95
  batch_size: 2048
  minibatch_size: 256
  update_epochs: 10
  clip_ratio: 0.2

llm:
  model_name: "gpt2"
  max_tokens: 512

simulator:
  num_nodes: 64
  num_queues: 1
  max_running_jobs: 512
  time_step: 60
