import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import warnings
import pymysql
from pymysql.err import OperationalError, ProgrammingError
import time
import random
warnings.filterwarnings("ignore")

# ===================== 1. MySQLè¿æ¥ï¼ˆæ ¸å¿ƒä¼˜åŒ–ï¼šéšæœºæŠ½æ ·+æ—¶é—´èŒƒå›´ï¼‰ =====================
class HPCMySQLConnector:
    def __init__(
            self,
            host: str = "localhost",
            port: int = 3307,
            user: str = "root",
            password: str = "123456",
            database: str = "xiyoudata",
            sample_size: int = 50000,  # éšæœºæŠ½æ ·æ€»æ•°
            timeout: int = 600
    ):
        self.host = host
        self.port = port
        self.user = user
        self.password = password
        self.database = database
        self.sample_size = sample_size  # æ”¹ä¸ºæŠ½æ ·æ€»æ•°ï¼Œè€Œéåˆ†æ‰¹å¤§å°
        self.timeout = timeout
        self.conn = None

    def connect(self) -> bool:
        try:
            self.conn = pymysql.connect(
                host=self.host,
                port=self.port,
                user=self.user,
                password=self.password,
                database=self.database,
                charset="utf8mb4",
                connect_timeout=self.timeout,
                read_timeout=self.timeout,
                write_timeout=self.timeout
            )
            cursor = self.conn.cursor()
            cursor.execute(f"SET SESSION MAX_EXECUTION_TIME = {self.timeout * 1000};")
            cursor.close()
            print(f"âœ… æˆåŠŸè¿æ¥MySQLæ•°æ®åº“: {self.host}:{self.port}/{self.database}")
            print(f"âœ… è®¾ç½®ä¼šè¯æŸ¥è¯¢è¶…æ—¶ä¸º{self.timeout}ç§’ï¼ŒéšæœºæŠ½æ ·{self.sample_size}è¡Œ")
            return True
        except OperationalError as e:
            print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            try:
                self.conn = pymysql.connect(
                    host=self.host,
                    port=self.port,
                    user=self.user,
                    password=self.password,
                    database=self.database,
                    charset="utf8mb4"
                )
                print(f"âœ… é™çº§è¿æ¥æˆåŠŸï¼ˆå…³é—­è¶…æ—¶é…ç½®ï¼‰")
                return True
            except:
                return False
        except ProgrammingError as e:
            print(f"âŒ è¶…æ—¶é…ç½®å¤±è´¥ï¼ˆå¿½ç•¥ï¼‰: {e}")
            return True

    def random_sample_table(self, table_name: str) -> pd.DataFrame:
        """æ ¸å¿ƒä¼˜åŒ–ï¼šéšæœºæŠ½æ ·ï¼Œç¡®ä¿è¦†ç›–ä¸åŒæ—¶é—´çª—å£"""
        if not self.conn:
            raise ValueError("è¯·å…ˆè°ƒç”¨connect()è¿æ¥æ•°æ®åº“")

        # åªåŠ è½½å¿…è¦å­—æ®µ
        field_mapping = {
            "task_usage": "job_id, task_index, start_time, cpu_rate, canonical_memory_usage, disk_io_time, maximum_cpu_rate, sampled_cpu_usage, machine_id",
            "task_events": "job_id, task_index, priority, cpu_request, memory_request, disk_space_request",
            "job_events": "job_id",
            "machine_events": "*",
            "machine_attributes": "*",
            "task_constraints": "*"
        }
        select_cols = field_mapping.get(table_name, "*")

        try:
            # ========== å…³é”®ä¼˜åŒ–1ï¼šå…ˆè·å–æ—¶é—´èŒƒå›´ ==========
            if table_name == "task_usage" and "start_time" in select_cols:
                cursor = self.conn.cursor()
                # è·å–æ—¶é—´èŒƒå›´
                cursor.execute(f"SELECT MIN(start_time), MAX(start_time) FROM {table_name}")
                min_time, max_time = cursor.fetchone()
                print(f"ğŸ” {table_name} æ—¶é—´èŒƒå›´: [{min_time}, {max_time}]")

                # æŒ‰æ—¶é—´åˆ†ç‰‡æŠ½æ ·ï¼ˆç¡®ä¿è¦†ç›–ä¸åŒæ—¶é—´æ®µï¼‰
                time_bins = 10  # åˆ†æˆ10ä¸ªæ—¶é—´ç‰‡
                bin_size = (max_time - min_time) // time_bins if max_time > min_time else 1
                df_list = []

                for bin_idx in range(time_bins):
                    bin_start = min_time + bin_idx * bin_size
                    bin_end = bin_start + bin_size
                    # æ¯ä¸ªæ—¶é—´ç‰‡æŠ½æ ·éƒ¨åˆ†æ•°æ®
                    sample_per_bin = self.sample_size // time_bins

                    query = f"""
                        SELECT {select_cols} 
                        FROM {table_name} 
                        WHERE start_time >= {bin_start} AND start_time < {bin_end}
                        ORDER BY RAND()  # éšæœºæ’åº
                        LIMIT {sample_per_bin}
                    """
                    batch_df = pd.read_sql(query, self.conn)
                    if len(batch_df) > 0:
                        df_list.append(batch_df)
                        print(f"ğŸ”„ {table_name} æ—¶é—´ç‰‡{bin_idx}: æŠ½æ ·{len(batch_df)}è¡Œ [{bin_start}, {bin_end}]")

                # åˆå¹¶æ‰€æœ‰æ—¶é—´ç‰‡æ•°æ®
                if df_list:
                    final_df = pd.concat(df_list, ignore_index=True)
                    # å»é‡å¹¶æˆªæ–­åˆ°ç›®æ ‡æŠ½æ ·æ•°
                    final_df = final_df.drop_duplicates(subset=["job_id", "task_index", "start_time"])
                    final_df = final_df.head(self.sample_size)
                else:
                    # å¤‡ç”¨æ–¹æ¡ˆï¼šå…¨è¡¨éšæœºæŠ½æ ·
                    query = f"""
                        SELECT {select_cols} 
                        FROM {table_name} 
                        ORDER BY RAND()
                        LIMIT {self.sample_size}
                    """
                    final_df = pd.read_sql(query, self.conn)
            else:
                # étask_usageè¡¨ç›´æ¥éšæœºæŠ½æ ·
                query = f"""
                    SELECT {select_cols} 
                    FROM {table_name} 
                    ORDER BY RAND()
                    LIMIT {self.sample_size}
                """
                final_df = pd.read_sql(query, self.conn)

            # ========== å…³é”®ä¼˜åŒ–2ï¼šæ£€æŸ¥æ—¶é—´æˆ³å¤šæ ·æ€§ ==========
            if table_name == "task_usage" and "start_time" in final_df.columns:
                unique_times = final_df["start_time"].nunique()
                print(f"âœ… {table_name} æŠ½æ ·å®Œæˆï¼šå…±{len(final_df)}è¡Œï¼Œå”¯ä¸€æ—¶é—´æˆ³æ•°={unique_times}")
                if unique_times < 5:
                    print(f"âš ï¸ {table_name} æ—¶é—´æˆ³å¤šæ ·æ€§ä¸è¶³ï¼Œè¡¥å……éšæœºæ•°æ®")
                    # è¡¥å……æ›´å¤šéšæœºæ•°æ®
                    extra_query = f"""
                        SELECT {select_cols} 
                        FROM {table_name} 
                        ORDER BY RAND()
                        LIMIT {self.sample_size // 2}
                    """
                    extra_df = pd.read_sql(extra_query, self.conn)
                    final_df = pd.concat([final_df, extra_df], ignore_index=True).drop_duplicates()
                    final_df = final_df.head(self.sample_size)
                    print(f"âœ… {table_name} è¡¥å……åï¼šå…±{len(final_df)}è¡Œï¼Œå”¯ä¸€æ—¶é—´æˆ³æ•°={final_df['start_time'].nunique()}")
            else:
                print(f"âœ… åŠ è½½è¡¨ {table_name} æˆåŠŸï¼Œå…± {len(final_df)} è¡Œ")

            return final_df

        except Exception as e:
            print(f"âŒ éšæœºæŠ½æ ·{table_name}å¤±è´¥: {e}")
            # é™çº§æ–¹æ¡ˆï¼šæŒ‰åç§»é‡åŠ è½½ï¼ˆå…¼å®¹æ—§é€»è¾‘ï¼‰
            return self.batch_load_table(table_name, limit=self.sample_size)

    def batch_load_table(self, table_name: str, limit: int = None) -> pd.DataFrame:
        """é™çº§æ–¹æ¡ˆï¼šåˆ†æ‰¹åŠ è½½ï¼ˆä¿ç•™æ—§é€»è¾‘ï¼‰"""
        field_mapping = {
            "task_usage": "job_id, task_index, start_time, cpu_rate, canonical_memory_usage, disk_io_time, maximum_cpu_rate, sampled_cpu_usage, machine_id",
            "task_events": "job_id, task_index, priority, cpu_request, memory_request, disk_space_request",
            "job_events": "job_id",
            "machine_events": "*",
            "machine_attributes": "*",
            "task_constraints": "*"
        }
        select_cols = field_mapping.get(table_name, "*")

        df_list = []
        offset = 0
        max_rows = limit if limit else float('inf')

        while True:
            query = f"""
                SELECT {select_cols} 
                FROM {table_name} 
                ORDER BY (SELECT NULL)
                LIMIT 5000 OFFSET {offset}
            """
            try:
                cursor = self.conn.cursor(pymysql.cursors.SSDictCursor)
                cursor.execute(query)
                batch_data = cursor.fetchall()
                cursor.close()

                if not batch_data:
                    break

                batch_df = pd.DataFrame(batch_data)
                df_list.append(batch_df)
                offset += 5000
                print(f"ğŸ”„ åŠ è½½{table_name}ï¼šå·²åŠ è½½{offset}è¡Œï¼ˆå½“å‰æ‰¹æ¬¡{len(batch_df)}è¡Œï¼‰")

                if offset >= max_rows:
                    break

                time.sleep(0.05)
            except Exception as e:
                print(f"âŒ åˆ†æ‰¹åŠ è½½{table_name}å¤±è´¥ï¼ˆåç§»é‡{offset}ï¼‰: {e}")
                break

        if df_list:
            final_df = pd.concat(df_list, ignore_index=True)
            if limit and len(final_df) > limit:
                final_df = final_df.head(limit)
            print(f"âœ… åŠ è½½è¡¨ {table_name} æˆåŠŸï¼Œå…± {len(final_df)} è¡Œ")
            return final_df
        else:
            print(f"âŒ åŠ è½½è¡¨ {table_name} å¤±è´¥ï¼šæ— æ•°æ®")
            return pd.DataFrame()

    def load_all_tables(self) -> Dict[str, pd.DataFrame]:
        """åŠ è½½æ‰€æœ‰è¡¨ï¼ˆtask_usageç”¨éšæœºæŠ½æ ·ï¼Œå…¶ä»–è¡¨ç”¨åˆ†æ‰¹åŠ è½½ï¼‰"""
        tables = [
            "job_events", "task_events", "machine_events",
            "machine_attributes", "task_constraints", "task_usage"
        ]
        hpc_data = {}

        # å…ˆåŠ è½½å°è¡¨
        for table in ["job_events", "machine_events", "task_constraints", "task_events", "machine_attributes"]:
            hpc_data[table] = self.batch_load_table(table, limit=self.sample_size)

        # task_usageç”¨éšæœºæŠ½æ ·ï¼ˆæ ¸å¿ƒï¼‰
        hpc_data["task_usage"] = self.random_sample_table("task_usage")

        return hpc_data

    def close(self):
        if self.conn:
            self.conn.close()
            print("âœ… æ•°æ®åº“è¿æ¥å·²å…³é—­")

# ===================== 2. æ—¶åºçª—å£æ„å»ºï¼ˆæ¢å¤æ­£å¸¸é€»è¾‘ï¼Œç§»é™¤è™šæ‹Ÿçª—å£ï¼‰ =====================
class TimeWindowProcessor:
    def __init__(
            self,
            window_size: int = 10,
            slide_step: int = 5,
            seq_len: int = 50  # æ¢å¤æ­£å¸¸æ—¶åºé•¿åº¦
    ):
        self.window_size = window_size
        self.slide_step = slide_step
        self.seq_len = seq_len

    def create_time_windows(self, df: pd.DataFrame, time_col: str = "start_time") -> pd.DataFrame:
        if len(df) == 0:
            print("âš ï¸ ç©ºDataFrameï¼Œè·³è¿‡æ—¶é—´çª—å£æ„å»º")
            return df

        # æ£€æŸ¥æ—¶é—´åˆ—
        if time_col not in df.columns:
            print(f"âš ï¸ ç¼ºå°‘æ—¶é—´åˆ—{time_col}ï¼Œå¯ç”¨åˆ—ï¼š{df.columns.tolist()}")
            time_candidates = [col for col in df.columns if 'time' in col.lower() or 'timestamp' in col.lower()]
            if time_candidates:
                time_col = time_candidates[0]
                print(f"âœ… è‡ªåŠ¨åŒ¹é…æ—¶é—´åˆ—ï¼š{time_col}")
            else:
                print("âŒ æ— å¯ç”¨æ—¶é—´åˆ—ï¼Œè¿”å›åŸæ•°æ®")
                return df

        # å¤„ç†æ—¶é—´åˆ—
        df = df.copy()
        df[time_col] = pd.to_numeric(df[time_col], errors='coerce')
        df = df.dropna(subset=[time_col])

        if len(df) == 0:
            print("âš ï¸ æ—¶é—´åˆ—æ— æœ‰æ•ˆæ•°å€¼ï¼Œè¿”å›ç©ºæ•°æ®")
            return df

        # æ­£å¸¸ç”Ÿæˆæ—¶é—´çª—å£ï¼ˆæ¢å¤åŸé€»è¾‘ï¼‰
        min_time = df[time_col].min()
        max_time = df[time_col].max()
        time_span = max_time - min_time
        print(f"ğŸ” æ—¶é—´èŒƒå›´: [{min_time}, {max_time}], è·¨åº¦: {time_span}ç§’")

        window_bins = np.arange(min_time, max_time + self.window_size, self.slide_step)
        df["window_id"] = pd.cut(
            df[time_col],
            bins=window_bins,
            labels=False,
            include_lowest=True
        )
        df = df.dropna(subset=["window_id"])
        df["window_id"] = df["window_id"].astype(int)

        window_count = df["window_id"].nunique()
        print(f"âœ… ç”Ÿæˆæ—¶é—´çª—å£å®Œæˆï¼šæ€»çª—å£æ•°={window_count}, è¦†ç›–æ—¶é—´è·¨åº¦={time_span}ç§’")
        return df

    def build_task_time_series(self, task_usage: pd.DataFrame) -> Dict[Tuple[int, int], np.ndarray]:
        if len(task_usage) == 0:
            print("âš ï¸ task_usageä¸ºç©ºï¼Œè¿”å›ç©ºæ—¶åºæ•°æ®")
            return {}

        # å®šä¹‰æ ¸å¿ƒç‰¹å¾
        feat_cols = [
            "cpu_rate", "canonical_memory_usage", "disk_io_time",
            "maximum_cpu_rate", "sampled_cpu_usage"
        ]
        # æ£€æŸ¥ç‰¹å¾åˆ—
        missing_feats = [col for col in feat_cols if col not in task_usage.columns]
        if missing_feats:
            print(f"âš ï¸ ç¼ºå°‘æ—¶åºç‰¹å¾åˆ—ï¼š{missing_feats}ï¼Œä»…ä½¿ç”¨å­˜åœ¨çš„åˆ—")
            feat_cols = [col for col in feat_cols if col in task_usage.columns]
            if not feat_cols:
                print("âŒ æ— å¯ç”¨æ—¶åºç‰¹å¾åˆ—ï¼Œè¿”å›ç©ºæ•°æ®")
                return {}

        # æ£€æŸ¥å…³é”®åˆ—
        for col in ["job_id", "task_index"]:
            if col not in task_usage.columns:
                print(f"âŒ ç¼ºå°‘å…³é”®åˆ—{col}ï¼Œæ— æ³•æ„å»ºæ—¶åºæ•°æ®")
                return {}

        # ç”Ÿæˆçª—å£
        task_usage = self.create_time_windows(task_usage)
        if len(task_usage) == 0:
            return {}

        # çª—å£èšåˆ
        agg_dict = {col: "mean" for col in feat_cols}
        try:
            task_window_agg = task_usage.groupby(["job_id", "task_index", "window_id"]).agg(agg_dict).reset_index()
        except Exception as e:
            print(f"âŒ çª—å£èšåˆå¤±è´¥: {e}")
            return {}

        # æ„å»ºæ—¶åºåºåˆ—
        task_series = {}
        task_groups = task_window_agg.groupby(["job_id", "task_index"])

        for (job_id, task_index), group in task_groups:
            # æŒ‰çª—å£æ’åº
            group_sorted = group.sort_values("window_id")
            group_feats = group_sorted[feat_cols].values

            # åªä¿ç•™æ—¶åºé•¿åº¦è¶³å¤Ÿçš„Task
            if len(group_feats) >= self.seq_len:
                task_series[(job_id, task_index)] = group_feats[:self.seq_len]

        print(f"âœ… Taskæ—¶åºåºåˆ—æ„å»ºå®Œæˆï¼šæœ‰æ•ˆTaskæ•°={len(task_series)}, æ—¶åºé•¿åº¦={self.seq_len}")
        return task_series

# ===================== 3. æ•°æ®é¢„å¤„ç†ï¼ˆæ¢å¤æ­£å¸¸é€»è¾‘ï¼‰ =====================
class HPCDataPreprocessor:
    def __init__(
            self,
            seq_len: int = 50,
            window_size: int = 10,
            slide_step: int = 5
    ):
        self.seq_len = seq_len
        self.window_processor = TimeWindowProcessor(
            window_size=window_size,
            slide_step=slide_step,
            seq_len=seq_len
        )
        self.feature_cols = [
            "cpu_rate", "canonical_memory_usage", "disk_io_time",
            "maximum_cpu_rate", "sampled_cpu_usage"
        ]
        self.static_cols = [
            "priority", "cpu_request", "memory_request", "disk_space_request"
        ]
        self.scalers = {}

    def normalize_series(self, series: np.ndarray, col_name: str = None) -> np.ndarray:
        mean = series.mean()
        std = series.std() + 1e-8
        if col_name:
            self.scalers[col_name] = (mean, std)
        return (series - mean) / std

    def process_task_data(self, hpc_data: Dict[str, pd.DataFrame]) -> Tuple[torch.Tensor, torch.Tensor, List[Dict]]:
        print("\n--- æ•°æ®åŸºç¡€ç»Ÿè®¡ ---")
        for table_name, df in hpc_data.items():
            print(f"{table_name}: æ€»è¡Œæ•°={len(df)}, éç©ºè¡Œæ•°={len(df.dropna())}")

        task_usage = hpc_data["task_usage"].copy() if "task_usage" in hpc_data else pd.DataFrame()
        task_events = hpc_data["task_events"].copy() if "task_events" in hpc_data else pd.DataFrame()
        job_events = hpc_data["job_events"].copy() if "job_events" in hpc_data else pd.DataFrame()

        if len(task_usage) == 0:
            raise ValueError("task_usageè¡¨ä¸ºç©ºï¼Œæ— æ³•ç»§ç»­å¤„ç†ï¼")

        # ç§»é™¤è™šæ‹Ÿæ—¶åºç›¸å…³ä»£ç ï¼Œæ¢å¤æ­£å¸¸é€»è¾‘
        print("\n--- æ„å»ºTaskæ—¶åºçª—å£ ---")
        task_series_dict = self.window_processor.build_task_time_series(task_usage)
        if len(task_series_dict) == 0:
            raise ValueError("æ— æœ‰æ•ˆæ—¶åºæ•°æ®ï¼è¯·æ£€æŸ¥æ—¶é—´çª—å£é…ç½®æˆ–æ•°æ®")

        print("\n--- é™æ€ç‰¹å¾å¤„ç† ---")
        static_feat_available = len(task_events) > 0
        if static_feat_available:
            for col in self.static_cols:
                if col in task_events.columns:
                    mean_val = task_events[col].mean()
                    task_events[col] = task_events[col].fillna(mean_val)
                    print(f"å¡«å……task_events.{col}ç¼ºå¤±å€¼ï¼Œå‡å€¼={mean_val:.4f}")
        else:
            print("âš ï¸ task_eventsä¸ºç©ºï¼Œé™æ€ç‰¹å¾ä½¿ç”¨é»˜è®¤å€¼")

        task_list = []
        task_metas = []
        job_ids = job_events["job_id"].dropna().unique() if len(job_events) > 0 else []
        job_id_map = {jid: idx for idx, jid in enumerate(job_ids)} if len(job_ids) > 0 else {}

        processed_count = 0
        skipped_count = 0

        for (job_id, task_index), ts_data in task_series_dict.items():
            try:
                ts_data_norm = np.zeros_like(ts_data)
                for i, col in enumerate(self.feature_cols[:ts_data.shape[1]]):
                    ts_data_norm[:, i] = self.normalize_series(ts_data[:, i], col)

                static_data = np.zeros(len(self.static_cols))
                if static_feat_available and "job_id" in task_events.columns and "task_index" in task_events.columns:
                    static_match = task_events[
                        (task_events["job_id"] == job_id) & (task_events["task_index"] == task_index)
                        ]
                    if len(static_match) > 0:
                        for i, col in enumerate(self.static_cols):
                            if col in static_match.columns:
                                static_data[i] = static_match[col].iloc[0]
                        static_data = self.normalize_series(static_data)

                static_repeated = np.tile(static_data, (self.seq_len, 1))
                task_feat = np.concatenate([ts_data_norm, static_repeated], axis=1)
                task_list.append(task_feat)

                machine_id = task_usage[
                    (task_usage["job_id"] == job_id) & (task_usage["task_index"] == task_index)
                    ]["machine_id"].iloc[0] if ("machine_id" in task_usage.columns and len(task_usage) > 0) else -1
                job_id_mapped = job_id_map.get(job_id, -1)

                task_metas.append({
                    "job_id": job_id_mapped,
                    "task_index": task_index,
                    "machine_id": machine_id,
                    "priority": static_data[0],
                    "cpu_request": static_data[1],
                    "memory_request": static_data[2],
                    "disk_request": static_data[3],
                    "raw_job_id": job_id
                })
                processed_count += 1
            except Exception as e:
                skipped_count += 1
                continue

        print(f"\n--- Taskå¤„ç†ç»Ÿè®¡ ---")
        print(f"æˆåŠŸå¤„ç†Taskæ•°: {processed_count}")
        print(f"è·³è¿‡Taskæ•°: {skipped_count}")

        if processed_count == 0:
            raise ValueError(f"æ— æœ‰æ•ˆTaskæ•°æ®ï¼æˆåŠŸå¤„ç†={processed_count}, è·³è¿‡={skipped_count}")

        all_job_ids = [meta["job_id"] for meta in task_metas]
        unique_job_ids = sorted(list(set(all_job_ids)))
        num_jobs_final = len(unique_job_ids)
        job_id_final_map = {jid: idx for idx, jid in enumerate(unique_job_ids)}

        job_mask = torch.zeros(num_jobs_final, len(task_list))
        for task_idx, meta in enumerate(task_metas):
            job_idx = job_id_final_map[meta["job_id"]]
            job_mask[job_idx, task_idx] = 1.0

        model_input = torch.tensor(np.array(task_list), dtype=torch.float32)
        print(f"\n--- é¢„å¤„ç†ç»“æœ ---")
        print(f"æ¨¡å‹è¾“å…¥å½¢çŠ¶: {model_input.shape} [num_tasks, seq_len, feat_dim]")
        print(f"Job-TaskçŸ©é˜µå½¢çŠ¶: {job_mask.shape} [num_jobs, num_tasks]")
        return model_input, job_mask, task_metas

# ===================== 4. Transformeræ¨¡å‹ï¼ˆæ¢å¤æ­£å¸¸å¤æ‚åº¦ï¼‰ =====================
class TransformerEncoder(nn.Module):
    def __init__(
            self,
            input_feat_dim: int,
            d_model: int = 128,
            num_heads: int = 4,
            num_layers: int = 2,
            dropout: float = 0.1,
            seq_len: int = 50
    ):
        super().__init__()
        self.input_proj = nn.Linear(input_feat_dim, d_model)
        self.pos_encoding = nn.Parameter(torch.randn(1, seq_len, d_model))
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=num_heads, dim_feedforward=d_model*4,
            dropout=dropout, batch_first=True, activation="gelu"
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.layer_norm = nn.LayerNorm(d_model)

    def forward(self, x: torch.Tensor, job_mask: torch.Tensor = None, aggregate_job: bool = False) -> torch.Tensor:
        x = self.input_proj(x) + self.pos_encoding[:, :x.shape[1], :]
        x = self.transformer(x)
        x = x.mean(dim=1)

        if aggregate_job and job_mask is not None:
            x = torch.matmul(job_mask, x) / job_mask.sum(dim=1, keepdim=True).clamp(min=1e-8)

        return self.layer_norm(x)

class HPCAutoencoder(nn.Module):
    def __init__(self, input_dim: int = 128, latent_dim: int = 32, dropout: float = 0.1):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, input_dim//2),
            nn.LayerNorm(input_dim//2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(input_dim//2, latent_dim),
            nn.LayerNorm(latent_dim)
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, input_dim//2),
            nn.LayerNorm(input_dim//2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(input_dim//2, input_dim)
        )

    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        latent = self.encoder(x)
        recon = self.decoder(latent)
        return recon, latent

class TransAE(nn.Module):
    def __init__(
            self,
            input_feat_dim: int,
            seq_len: int = 50,
            d_model: int = 128,
            num_heads: int = 4,
            num_layers: int = 2,
            latent_dim: int = 32,
            dropout: float = 0.1
    ):
        super().__init__()
        self.transformer = TransformerEncoder(
            input_feat_dim=input_feat_dim,
            d_model=d_model,
            num_heads=num_heads,
            num_layers=num_layers,
            dropout=dropout,
            seq_len=seq_len
        )
        self.ae = HPCAutoencoder(input_dim=d_model, latent_dim=latent_dim, dropout=dropout)

    def forward(
            self,
            x: torch.Tensor,
            job_mask: torch.Tensor = None,
            aggregate_job: bool = False
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        trans_feat = self.transformer(x, job_mask, aggregate_job=aggregate_job)
        recon_feat, latent_feat = self.ae(trans_feat)
        return recon_feat, latent_feat, trans_feat

# ===================== 5. è‡ªå®šä¹‰èšç±»ï¼ˆæ— ä¿®æ”¹ï¼‰ =====================
class HPCCustomKMeans:
    def __init__(self, n_clusters: int = 5, hpc_weight: float = 0.2):
        self.n_clusters = n_clusters
        self.hpc_weight = hpc_weight
        self.kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        self.centroids = None
        self.labels = None

    def custom_distance(self, feat1: np.ndarray, feat2: np.ndarray, meta1: Dict, meta2: Dict) -> float:
        feat_dist = np.linalg.norm(feat1 - feat2)
        res1 = np.array([meta1["cpu_request"], meta1["memory_request"], meta1["disk_request"]])
        res2 = np.array([meta2["cpu_request"], meta2["memory_request"], meta2["disk_request"]])
        res_dist = 1 - np.dot(res1, res2) / (np.linalg.norm(res1)*np.linalg.norm(res2) + 1e-8)
        job_dist = 0 if meta1["job_id"] == meta2["job_id"] else 1.0
        return 0.5*feat_dist + 0.3*res_dist + 0.2*job_dist

    def fit(self, latent_feat: np.ndarray, task_metas: List[Dict]):
        self.kmeans.fit(latent_feat)
        self.labels = self.kmeans.labels_
        self.centroids = self.kmeans.cluster_centers_

        max_iter = 10
        for _ in range(max_iter):
            new_labels = []
            for i in range(len(latent_feat)):
                distances = []
                for j in range(self.n_clusters):
                    cluster_meta = [task_metas[k] for k in range(len(self.labels)) if self.labels[k] == j]
                    if not cluster_meta:
                        centroid_feat = self.centroids[j]
                    else:
                        centroid_feat = np.mean([latent_feat[k] for k in range(len(self.labels)) if self.labels[k] == j], axis=0)
                    distances.append(self.custom_distance(latent_feat[i], centroid_feat, task_metas[i], cluster_meta[0] if cluster_meta else task_metas[i]))
                new_labels.append(np.argmin(distances))

            new_centroids = []
            for c in range(self.n_clusters):
                cluster_feats = latent_feat[np.array(new_labels) == c]
                if len(cluster_feats) == 0:
                    new_centroids.append(self.centroids[c])
                else:
                    new_centroids.append(np.mean(cluster_feats, axis=0))

            if np.array_equal(self.labels, new_labels):
                break
            self.labels = new_labels
            self.centroids = np.array(new_centroids)

    def evaluate(self, latent_feat: np.ndarray, task_metas: List[Dict]) -> Dict:
        intra_res_consist = []
        for c in range(self.n_clusters):
            cluster_metas = [task_metas[i] for i in range(len(self.labels)) if self.labels[i] == c]
            if len(cluster_metas) < 2:
                intra_res_consist.append(1.0)
                continue
            cpu_var = np.var([m["cpu_request"] for m in cluster_metas])
            mem_var = np.var([m["memory_request"] for m in cluster_metas])
            intra_res_consist.append(1 - (cpu_var + mem_var) / 2)

        sil_score = silhouette_score(latent_feat, self.labels) if len(np.unique(self.labels)) > 1 else 0.0

        job_cluster_ratio = []
        for job_id in np.unique([m["job_id"] for m in task_metas]):
            job_tasks = [i for i, m in enumerate(task_metas) if m["job_id"] == job_id]
            if len(job_tasks) == 0:
                continue
            job_labels = [self.labels[i] for i in job_tasks]
            max_count = max([job_labels.count(l) for l in np.unique(job_labels)])
            job_cluster_ratio.append(max_count / len(job_labels))

        return {
            "intra_resource_consistency": np.mean(intra_res_consist),
            "silhouette_score": sil_score,
            "job_cohesion": np.mean(job_cluster_ratio) if job_cluster_ratio else 0.0,
            "total_score": 0.4*np.mean(intra_res_consist) + 0.3*sil_score + 0.3*np.mean(job_cluster_ratio)
        }

# ===================== 6. ä¸»æµç¨‹ï¼ˆæ¢å¤æ­£å¸¸é…ç½®ï¼‰ =====================
def main():
    # ===================== æ ¸å¿ƒé…ç½®ï¼ˆæ¢å¤æ­£å¸¸ï¼‰ =====================
    WINDOW_SIZE = 10
    SLIDE_STEP = 5
    SEQ_LEN = 50           # æ¢å¤æ­£å¸¸æ—¶åºé•¿åº¦
    LATENT_DIM = 32        # æ¢å¤æ­£å¸¸ç‰¹å¾ç»´åº¦
    D_MODEL = 128          # æ¢å¤æ­£å¸¸æ¨¡å‹å¤æ‚åº¦
    NUM_CLUSTERS = 5       # æ¢å¤æ­£å¸¸èšç±»æ•°
    EPOCHS = 10            # æ¢å¤æ­£å¸¸è®­ç»ƒè½®æ•°
    SAMPLE_SIZE = 100000   # éšæœºæŠ½æ ·10ä¸‡è¡Œï¼ˆè¦†ç›–æ›´å¤šæ—¶é—´çª—å£ï¼‰
    DB_HOST = "localhost"
    BATCH_SIZE = 256       # æ¢å¤æ­£å¸¸æ‰¹æ¬¡å¤§å°
    DB_TIMEOUT = 600       # æ¢å¤æ­£å¸¸è¶…æ—¶æ—¶é—´

    # ===================== 1. è¿æ¥MySQLå¹¶åŠ è½½æ•°æ® =====================
    print("=== 1. è¿æ¥MySQLæ•°æ®åº“ ===")
    connector = HPCMySQLConnector(
        host=DB_HOST,
        port=3307,
        user="root",
        password="123456",
        database="xiyoudata",
        sample_size=SAMPLE_SIZE,
        timeout=DB_TIMEOUT
    )
    if not connector.connect():
        print("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        return

    print("\n=== 2. éšæœºæŠ½æ ·åŠ è½½HPCæ•°æ®è¡¨ï¼ˆ10ä¸‡è¡Œï¼‰ ===")
    hpc_data = connector.load_all_tables()
    connector.close()

    # ===================== 2. æ•°æ®é¢„å¤„ç† =====================
    print("\n=== 3. æ•°æ®é¢„å¤„ç†ï¼ˆæ—¶åºçª—å£+ç‰¹å¾æ•´åˆï¼‰ ===")
    preprocessor = HPCDataPreprocessor(
        seq_len=SEQ_LEN,
        window_size=WINDOW_SIZE,
        slide_step=SLIDE_STEP
    )
    try:
        model_input, job_mask, task_metas = preprocessor.process_task_data(hpc_data)
    except ValueError as e:
        print(f"âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}")
        return

    # ===================== 3. åˆå§‹åŒ–æ¨¡å‹ =====================
    print("\n=== 4. åˆå§‹åŒ–æ¨¡å‹ä¸åˆ†æ‰¹è®­ç»ƒ ===")
    input_feat_dim = model_input.shape[-1]
    model = TransAE(
        input_feat_dim=input_feat_dim,
        seq_len=SEQ_LEN,
        d_model=D_MODEL,
        num_heads=4,
        num_layers=2,
        latent_dim=LATENT_DIM,
        dropout=0.1
    )
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)
    loss_fn = nn.MSELoss()

    num_samples = model_input.shape[0]
    num_batches = (num_samples + BATCH_SIZE - 1) // BATCH_SIZE
    print(f"æ€»æ ·æœ¬æ•°: {num_samples}, æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}, æ€»æ‰¹æ¬¡: {num_batches}")

    # ===================== 4. æ¨¡å‹è®­ç»ƒ =====================
    model.train()
    for epoch in range(EPOCHS):
        epoch_loss = 0.0
        for batch_idx in range(num_batches):
            start_idx = batch_idx * BATCH_SIZE
            end_idx = min((batch_idx + 1) * BATCH_SIZE, num_samples)
            batch_input = model_input[start_idx:end_idx]

            recon_feat, latent_feat, trans_feat = model(batch_input, aggregate_job=False)
            loss = loss_fn(recon_feat, trans_feat)

            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            epoch_loss += loss.item() * (end_idx - start_idx)

        avg_loss = epoch_loss / num_samples
        if (epoch+1) % 2 == 0:
            print(f"Epoch {epoch+1}/{EPOCHS} | Average Loss: {avg_loss:.4f}")

    # ===================== 5. æå–ç‰¹å¾ =====================
    print("\n=== 5. æå–ä½ç»´ç‰¹å¾ ===")
    model.eval()
    latent_feat_list = []

    with torch.no_grad():
        for batch_idx in range(num_batches):
            start_idx = batch_idx * BATCH_SIZE
            end_idx = min((batch_idx + 1) * BATCH_SIZE, num_samples)
            batch_input = model_input[start_idx:end_idx]

            _, latent_feat, _ = model(batch_input, aggregate_job=False)
            latent_feat_list.append(latent_feat.cpu().numpy())

    if latent_feat_list:
        task_latent_np = np.concatenate(latent_feat_list, axis=0)
        print(f"âœ… Taskçº§ä½ç»´ç‰¹å¾å½¢çŠ¶: {task_latent_np.shape}")

        # Jobçº§èšåˆ
        if job_mask.shape[1] == len(task_latent_np):
            task_latent_tensor = torch.tensor(task_latent_np, dtype=torch.float32)
            with torch.no_grad():
                job_latent_tensor = torch.matmul(job_mask, task_latent_tensor) / job_mask.sum(dim=1, keepdim=True).clamp(min=1e-8)
            job_latent_np = job_latent_tensor.cpu().numpy()
            print(f"âœ… Jobçº§ä½ç»´ç‰¹å¾å½¢çŠ¶: {job_latent_np.shape}")
            cluster_feat = job_latent_np
        else:
            cluster_feat = task_latent_np
            print("âš ï¸ æ— æ³•èšåˆJobçº§ç‰¹å¾ï¼Œä½¿ç”¨Taskçº§ç‰¹å¾èšç±»")

        # ===================== 6. è‡ªå®šä¹‰èšç±» =====================
        print("\n=== 6. è‡ªå®šä¹‰HPCèšç±» ===")
        if len(cluster_feat) >= NUM_CLUSTERS:
            # PCAé™ç»´ï¼ˆå¯é€‰ï¼‰
            if cluster_feat.shape[1] > 50:
                from sklearn.decomposition import PCA
                pca = PCA(n_components=50, random_state=42)
                cluster_feat = pca.fit_transform(cluster_feat)
                print(f"PCAé™ç»´åç‰¹å¾å½¢çŠ¶: {cluster_feat.shape}")

            hpc_kmeans = HPCCustomKMeans(n_clusters=NUM_CLUSTERS)
            hpc_kmeans.fit(cluster_feat, task_metas)

            # èšç±»è¯„ä»·
            metrics = hpc_kmeans.evaluate(cluster_feat, task_metas)
            print("âœ… èšç±»è¯„ä»·æŒ‡æ ‡:")
            for k, v in metrics.items():
                print(f"  {k}: {v:.4f}")

            # ç»“æœåˆ†æ
            print("\n=== 7. èšç±»ç»“æœåˆ†æ ===")
            for cluster_id in range(NUM_CLUSTERS):
                cluster_tasks = [task_metas[i] for i in range(len(hpc_kmeans.labels)) if hpc_kmeans.labels[i] == cluster_id]
                if not cluster_tasks:
                    continue
                avg_cpu = np.mean([t["cpu_request"] for t in cluster_tasks])
                avg_mem = np.mean([t["memory_request"] for t in cluster_tasks])
                job_count = len(set([t["raw_job_id"] for t in cluster_tasks]))
                print(f"èšç±» {cluster_id}:")
                print(f"  åŒ…å«Taskæ•°: {len(cluster_tasks)}")
                print(f"  æ¶‰åŠJobæ•°: {job_count}")
                print(f"  å¹³å‡CPUè¯·æ±‚: {avg_cpu:.4f}")
                print(f"  å¹³å‡å†…å­˜è¯·æ±‚: {avg_mem:.4f}")
        else:
            print("âš ï¸ æ•°æ®é‡ä¸è¶³ï¼Œè·³è¿‡èšç±»")
    else:
        print("âš ï¸ æ— ä½ç»´ç‰¹å¾ï¼Œè·³è¿‡èšç±»")

    print("\n=== æµç¨‹æ‰§è¡Œå®Œæˆ ===")

if __name__ == "__main__":
    main()
