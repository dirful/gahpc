"""
ä¿®å¤ç‰ˆä¸»ç¨‹åº
"""
import argparse
import pickle
import json
import os
import sys
from datetime import datetime
import pandas as pd
import numpy as np

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from database import DatabaseConnector
from feature_engine import FeatureEngine
from param_optimizer import ParamOptimizer
from clustering import MultiViewClustering
from visualization import VisualizationSystem
from config import EXPERIMENT_CONFIG, CLUSTERING_CONFIG, VISUALIZATION_CONFIG

def main_fixed():
    """ä¿®å¤ç‰ˆä¸»å‡½æ•°"""
    print("="*80)
    print("HPCå·¥ä½œè´Ÿè½½åˆ†æä¸åˆ†ç±»ç³»ç»Ÿ - ä¿®å¤ç‰ˆ")
    print("="*80)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = f"hpc_results_{timestamp}"
    os.makedirs(output_dir, exist_ok=True)

    # åˆ›å»ºå­ç›®å½•
    for subdir in ['raw_data', 'features', 'clustering', 'visualizations', 'reports']:
        os.makedirs(os.path.join(output_dir, subdir), exist_ok=True)

    print(f"è¾“å‡ºç›®å½•: {output_dir}")

    # æ­¥éª¤1: æ•°æ®åŠ è½½
    print("\n" + "="*60)
    print("æ­¥éª¤1: æ•°æ®åŠ è½½")
    print("="*60)

    db = DatabaseConnector()
    if not db.connect():
        print("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥!")
        return

    try:
        # åŠ è½½æ•°æ®
        print("åŠ è½½ä»»åŠ¡æ—¶é—´åºåˆ—æ•°æ®...")
        df = db.load_task_time_series(num_tasks=100, min_samples=10)

        if df is None or len(df) == 0:
            print("å°è¯•åŠ è½½é«˜é¢‘ä»»åŠ¡...")
            df = db.load_high_frequency_tasks(num_tasks=50, frequency_threshold=20)

        if df is None or len(df) == 0:
            print("âŒ æ•°æ®åŠ è½½å¤±è´¥!")
            return

        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ!")
        print(f"  æ€»æ•°æ®ç‚¹: {len(df)}")
        print(f"  ä»»åŠ¡æ•°é‡: {df.groupby(['job_id', 'task_index']).ngroups}")

        # ä¿å­˜åŸå§‹æ•°æ®
        raw_path = os.path.join(output_dir, 'raw_data', 'task_data.csv')
        df.to_csv(raw_path, index=False)
        print(f"  åŸå§‹æ•°æ®å·²ä¿å­˜: {raw_path}")

    finally:
        db.disconnect()

    # æ­¥éª¤2: ç‰¹å¾å·¥ç¨‹
    print("\n" + "="*60)
    print("æ­¥éª¤2: ç‰¹å¾å·¥ç¨‹")
    print("="*60)

    feature_engine = FeatureEngine()

    print("æå–ç‰¹å¾...")
    features_df = feature_engine.extract_multi_view_features(df, max_tasks=200)

    if features_df is None or len(features_df) == 0:
        print("âŒ ç‰¹å¾æå–å¤±è´¥!")
        return

    # ä¿å­˜ç‰¹å¾
    features_path = os.path.join(output_dir, 'features', 'task_features.csv')
    features_df.to_csv(features_path, index=False)

    print(f"âœ… ç‰¹å¾æå–å®Œæˆ!")
    print(f"  ç‰¹å¾ç»´åº¦: {features_df.shape}")
    print(f"  ç‰¹å¾å·²ä¿å­˜: {features_path}")

    # æ­¥éª¤3: èšç±»åˆ†æ
    print("\n" + "="*60)
    print("æ­¥éª¤3: èšç±»åˆ†æ")
    print("="*60)

    clustering = MultiViewClustering({'n_clusters': 5})

    # å‡†å¤‡ç‰¹å¾
    features = clustering.prepare_features(features_df)
    features_scaled = clustering.standardize_features()

    # é™ç»´
    print("æ‰§è¡Œé™ç»´...")
    embeddings = clustering.dimensionality_reduction(method='umap')

    # èšç±»
    print("æ‰§è¡Œèšç±»...")
    labels = clustering.perform_clustering(method='kmeans')

    # åˆ†æèšç±»
    print("åˆ†æèšç±»ç»“æœ...")
    cluster_stats_df, cluster_profiles = clustering.analyze_clusters(features_df)

    if cluster_stats_df is None:
        print("âŒ èšç±»åˆ†æå¤±è´¥!")
        return

    # ä¿å­˜èšç±»ç»“æœ
    clustering_dir = os.path.join(output_dir, 'clustering')

    # å¸¦æ ‡ç­¾çš„ç‰¹å¾æ•°æ®
    features_with_labels = features_df.copy()
    features_with_labels['cluster'] = labels[:len(features_df)]
    features_with_labels_path = os.path.join(clustering_dir, 'features_with_clusters.csv')
    features_with_labels.to_csv(features_with_labels_path, index=False)

    # èšç±»ç»Ÿè®¡
    cluster_stats_path = os.path.join(clustering_dir, 'cluster_statistics.csv')
    cluster_stats_df.to_csv(cluster_stats_path, index=False)

    print(f"âœ… èšç±»åˆ†æå®Œæˆ!")
    print(f"  èšç±»æ•°é‡: {len(cluster_stats_df)}")

    # æ˜¾ç¤ºèšç±»åˆ†å¸ƒ
    print(f"\nğŸ“Š èšç±»åˆ†å¸ƒ:")
    for _, row in cluster_stats_df.iterrows():
        cluster_id = row['cluster_id']
        size = row['size']
        percentage = row['percentage']
        print(f"  èšç±» {cluster_id}: {size} ä¸ªä»»åŠ¡ ({percentage:.1f}%)")

    # æ­¥éª¤4: å¯è§†åŒ–
    print("\n" + "="*60)
    print("æ­¥éª¤4: å¯è§†åŒ–")
    print("="*60)

    viz_dir = os.path.join(output_dir, 'visualizations')
    os.makedirs(viz_dir, exist_ok=True)

    visualization = VisualizationSystem(output_dir=viz_dir)

    try:
        print("åˆ›å»ºå¯è§†åŒ–å›¾è¡¨...")

        # 1. èšç±»æ•£ç‚¹å›¾
        print("  åˆ›å»ºèšç±»æ•£ç‚¹å›¾...")
        visualization.create_cluster_scatter(embeddings, labels)

        # 2. å°æç´å›¾
        print("  åˆ›å»ºç‰¹å¾åˆ†å¸ƒå°æç´å›¾...")
        visualization.create_violin_plots(features_with_labels)

        # 3. çƒ­åŠ›å›¾
        print("  åˆ›å»ºèšç±»çƒ­åŠ›å›¾...")
        visualization.create_heatmap(features_with_labels)

        # 4. æ—¶é—´åºåˆ—å›¾
        print("  åˆ›å»ºæ—¶é—´åºåˆ—å åŠ å›¾...")
        # å‡†å¤‡ä»»åŠ¡IDåˆ—è¡¨
        job_task_ids = []
        for _, row in features_df.iterrows():
            if 'job_id' in row and 'task_index' in row:
                job_task_ids.append((row['job_id'], row['task_index']))

        if job_task_ids:
            visualization.create_time_series_overlay(df, labels, job_task_ids, n_samples=2)

        # 5. ä»ªè¡¨æ¿
        print("  åˆ›å»ºèšç±»ä»ªè¡¨æ¿...")
        visualization.create_dashboard(cluster_stats_df, cluster_profiles)

        # ä¿å­˜æ‰€æœ‰å›¾å½¢
        visualization.save_all_figures()

        print(f"âœ… å¯è§†åŒ–å®Œæˆ!")

    except Exception as e:
        print(f"âš ï¸ å¯è§†åŒ–é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

    # æ­¥éª¤5: ç”ŸæˆæŠ¥å‘Š
    print("\n" + "="*60)
    print("æ­¥éª¤5: ç”ŸæˆæŠ¥å‘Š")
    print("="*60)

    reports_dir = os.path.join(output_dir, 'reports')
    report_path = os.path.join(reports_dir, 'analysis_report.txt')

    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("="*60 + "\n")
        f.write("HPCå·¥ä½œè´Ÿè½½åˆ†æä¸åˆ†ç±»æŠ¥å‘Š\n")
        f.write("="*60 + "\n\n")

        f.write(f"åˆ†ææ—¶é—´: {timestamp}\n")
        f.write(f"è¾“å‡ºç›®å½•: {output_dir}\n\n")

        f.write("1. æ•°æ®æ¦‚å†µ\n")
        f.write("-"*40 + "\n")
        f.write(f"   æ€»æ•°æ®ç‚¹: {len(df)}\n")
        f.write(f"   ä»»åŠ¡æ•°é‡: {features_df.shape[0]}\n")
        f.write(f"   ç‰¹å¾æ•°é‡: {features_df.shape[1]}\n\n")

        f.write("2. èšç±»ç»“æœ\n")
        f.write("-"*40 + "\n")
        for _, row in cluster_stats_df.iterrows():
            cluster_id = row['cluster_id']
            size = row['size']
            percentage = row['percentage']
            f.write(f"   èšç±» {cluster_id}: {size} ä¸ªä»»åŠ¡ ({percentage:.1f}%)\n")
        f.write("\n")

        f.write("3. å·¥ä½œè´Ÿè½½ç±»å‹\n")
        f.write("-"*40 + "\n")
        for cluster_id, profile in cluster_profiles.items():
            f.write(f"\n   ç±»å‹ {cluster_id}:\n")
            f.write(f"     ä¸»å¯¼èµ„æº: {profile.get('dominant_resource', 'N/A')}\n")
            f.write(f"     è¡Œä¸ºæ¨¡å¼: {profile.get('behavior_type', 'N/A')}\n")
            f.write(f"     æ³¢åŠ¨ç‰¹æ€§: {profile.get('volatility_level', 'N/A')}\n")
        f.write("\n")

        f.write("4. ç”Ÿæˆæ–‡ä»¶\n")
        f.write("-"*40 + "\n")

        for root, dirs, files in os.walk(output_dir):
            level = root.replace(output_dir, '').count(os.sep)
            indent = ' ' * 4 * level
            f.write(f"{indent}{os.path.basename(root)}/\n")

            subindent = ' ' * 4 * (level + 1)
            for file in files:
                f.write(f"{subindent}{file}\n")

    print(f"âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ!")
    print(f"  æŠ¥å‘Šæ–‡ä»¶: {report_path}")

    # ä¿å­˜æœ€ç»ˆç»“æœ
    final_results = {
        'timestamp': timestamp,
        'output_dir': output_dir,
        'features_df_shape': features_df.shape,
        'cluster_stats': cluster_stats_df.to_dict('records'),
        'cluster_profiles': cluster_profiles
    }

    results_path = os.path.join(output_dir, 'final_results.json')
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(final_results, f, indent=2, default=str)

    print(f"âœ… æœ€ç»ˆç»“æœå·²ä¿å­˜: {results_path}")

    print("\n" + "="*80)
    print("âœ… åˆ†æå®Œæˆ!")
    print(f"æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {output_dir}")
    print("="*80)

if __name__ == "__main__":
    main_fixed()