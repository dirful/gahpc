"""
å®Œæ•´çš„HPCå·¥ä½œè´Ÿè½½åˆ†æä¸»ç¨‹åº
"""
import argparse
import pickle
import json
import os
from datetime import datetime
import pandas as pd
import numpy as np



from database import DatabaseConnector
from feature_engine import FeatureEngine
from param_optimizer import ParamOptimizer
from clustering import MultiViewClustering
from visualization import VisualizationSystem
from config import EXPERIMENT_CONFIG, CLUSTERING_CONFIG, VISUALIZATION_CONFIG

class HPCWorkloadAnalyzer:
    def __init__(self, config=None):
        self.config = config or EXPERIMENT_CONFIG
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.output_dir = f"hpc_results_{self.timestamp}"
        os.makedirs(self.output_dir, exist_ok=True)

        self.results = {
            'config': self.config,
            'timestamp': self.timestamp,
            'output_dir': self.output_dir
        }

    def setup_directories(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„"""
        subdirs = ['raw_data', 'features', 'clustering', 'visualizations', 'reports']
        for subdir in subdirs:
            os.makedirs(os.path.join(self.output_dir, subdir), exist_ok=True)
        return self.output_dir

    def run_data_loading(self, sample_size=None):
        """æ­¥éª¤1: æ•°æ®åŠ è½½"""
        print("\n" + "="*60)
        print("æ­¥éª¤1: æ•°æ®åŠ è½½")
        print("="*60)

        db = DatabaseConnector()
        if not db.connect():
            print("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥!")
            return None

        try:
            # ä½¿ç”¨é…ç½®çš„æ ·æœ¬å¤§å°
            sample_size = sample_size or self.config['sample_size']

            print(f"åŠ è½½æ•°æ® (æ ·æœ¬å¤§å°: {sample_size})...")

            # æ–¹æ³•1: å°è¯•åŠ è½½é«˜é¢‘ä»»åŠ¡
            df = db.load_high_frequency_tasks(
                num_tasks=min(50, sample_size // 100),
                frequency_threshold=20
            )

            if df is None or len(df) == 0:
                print("é«˜é¢‘ä»»åŠ¡åŠ è½½å¤±è´¥ï¼Œå°è¯•æ™®é€šä»»åŠ¡...")
                df = db.load_task_time_series(
                    num_tasks=min(100, sample_size // 50),
                    min_samples=10
                )

            if df is None or len(df) == 0:
                print("âŒ æ•°æ®åŠ è½½å¤±è´¥!")
                return None

            # ä¿å­˜åŸå§‹æ•°æ®
            raw_data_path = os.path.join(self.output_dir, 'raw_data', 'task_data.csv')
            df.to_csv(raw_data_path, index=False)

            print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ!")
            print(f"  æ€»æ•°æ®ç‚¹: {len(df):,}")
            print(f"  å”¯ä¸€ä»»åŠ¡æ•°: {df.groupby(['job_id', 'task_index']).ngroups}")
            print(f"  åŸå§‹æ•°æ®å·²ä¿å­˜: {raw_data_path}")

            self.results['raw_data'] = df
            self.results['raw_data_path'] = raw_data_path

            return df

        finally:
            db.disconnect()

    def run_feature_engineering(self, df, optimize_params=True):
        """æ­¥éª¤2: ç‰¹å¾å·¥ç¨‹ä¸å‚æ•°ä¼˜åŒ–"""
        print("\n" + "="*60)
        print("æ­¥éª¤2: ç‰¹å¾å·¥ç¨‹ä¸å‚æ•°ä¼˜åŒ–")
        print("="*60)

        if df is None or len(df) == 0:
            print("âŒ è¾“å…¥æ•°æ®ä¸ºç©º!")
            return None

        feature_engine = FeatureEngine()

        # å‚æ•°ä¼˜åŒ–
        if optimize_params and self.config.get('optimize_params', True):
            print("æ‰§è¡Œå‚æ•°ä¼˜åŒ–...")

            # ä½¿ç”¨å°‘é‡æ•°æ®è¿›è¡Œå¿«é€Ÿä¼˜åŒ–
            print("ä½¿ç”¨å°‘é‡æ•°æ®è¿›è¡Œå‚æ•°ä¼˜åŒ–...")
            optimizer = ParamOptimizer(feature_engine)

            # ä¸ºäº†é€Ÿåº¦ï¼Œä½¿ç”¨æ›´å°‘çš„ä»»åŠ¡è¿›è¡Œä¼˜åŒ–
            optimization_df = df.copy()
            if len(df) > 50000:
                # é‡‡æ ·ç”¨äºä¼˜åŒ–
                optimization_df = df.sample(50000, random_state=42)

            best_params, best_score, opt_features = optimizer.grid_search(
                optimization_df,
                param_grid={
                    'cpu_weight': [0.3, 0.4, 0.5],
                    'mem_weight': [0.2, 0.3, 0.4],
                    'io_weight': [0.1, 0.2, 0.3],
                    'diff_weight': [0.05, 0.1, 0.15],
                    'volatility_weight': [0.1, 0.2, 0.3]
                }
            )

            if best_params:
                feature_engine.set_parameters(best_params)
                print(f"âœ… å‚æ•°ä¼˜åŒ–å®Œæˆ!")
                print(f"  æœ€ä½³å¾—åˆ†: {best_score:.4f}")
                print(f"  æœ€ä½³å‚æ•°: {best_params}")

                self.results['optimized_params'] = best_params
                self.results['optimization_score'] = best_score
                self.results['optimization_history'] = optimizer.history

                # ä¿å­˜ä¼˜åŒ–å†å²
                opt_history_path = os.path.join(self.output_dir, 'features', 'optimization_history.csv')
                pd.DataFrame(optimizer.history).to_csv(opt_history_path, index=False)

            else:
                print("âš ï¸ å‚æ•°ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
        else:
            print("è·³è¿‡å‚æ•°ä¼˜åŒ–ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")

        # æå–ç‰¹å¾ï¼ˆä½¿ç”¨æ‰€æœ‰æ•°æ®ï¼‰
        print("\næå–å¤šè§†å›¾ç‰¹å¾...")
        features_df = feature_engine.extract_multi_view_features(
            df,
            max_tasks=self.config.get('max_tasks', 2000)
        )

        if features_df is None or len(features_df) == 0:
            print("âŒ ç‰¹å¾æå–å¤±è´¥!")
            return None

        # ä¿å­˜ç‰¹å¾
        features_path = os.path.join(self.output_dir, 'features', 'task_features.csv')
        features_df.to_csv(features_path, index=False)

        # ç‰¹å¾ç»Ÿè®¡
        numeric_cols = features_df.select_dtypes(include=[np.number]).columns
        feature_stats = features_df[numeric_cols].describe()
        stats_path = os.path.join(self.output_dir, 'features', 'feature_statistics.csv')
        feature_stats.to_csv(stats_path)

        print(f"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆ!")
        print(f"  ç‰¹å¾ç»´åº¦: {features_df.shape}")
        print(f"  ç‰¹å¾å·²ä¿å­˜: {features_path}")
        print(f"  ç»Ÿè®¡ä¿¡æ¯: {stats_path}")

        self.results['features_df'] = features_df
        self.results['feature_engine'] = feature_engine

        return features_df

    def run_clustering(self, features_df, n_clusters=None):
        """æ­¥éª¤3: èšç±»åˆ†æ"""
        print("\n" + "="*60)
        print("æ­¥éª¤3: èšç±»åˆ†æ")
        print("="*60)

        if features_df is None or len(features_df) == 0:
            print("âŒ ç‰¹å¾æ•°æ®ä¸ºç©º!")
            return None

        # ä½¿ç”¨é…ç½®çš„èšç±»æ•°é‡
        n_clusters = n_clusters or self.config.get('n_clusters', CLUSTERING_CONFIG['n_clusters'])
        clustering_config = CLUSTERING_CONFIG.copy()
        clustering_config['n_clusters'] = n_clusters

        clustering = MultiViewClustering(clustering_config)

        print(f"æ‰§è¡Œ {n_clusters} ç±»èšç±»åˆ†æ...")

        # å‡†å¤‡ç‰¹å¾
        features = clustering.prepare_features(features_df)

        # æ ‡å‡†åŒ–
        features_scaled = clustering.standardize_features()

        # é™ç»´
        embeddings = clustering.dimensionality_reduction(
            method=clustering_config.get('dim_reduction_method', 'umap')
        )

        # æ‰§è¡Œèšç±»
        labels = clustering.perform_clustering(
            method=clustering_config.get('clustering_method', 'kmeans')
        )

        # åˆ†æèšç±»ç»“æœ
        cluster_stats_df, cluster_profiles = clustering.analyze_clusters(features_df)

        if cluster_stats_df is None:
            print("âŒ èšç±»åˆ†æå¤±è´¥!")
            return None

        # ä¿å­˜èšç±»ç»“æœ
        clustering_dir = os.path.join(self.output_dir, 'clustering')

        # å¸¦æ ‡ç­¾çš„ç‰¹å¾æ•°æ®
        features_with_labels = features_df.copy()
        features_with_labels['cluster'] = labels[:len(features_df)]
        features_with_labels_path = os.path.join(clustering_dir, 'features_with_clusters.csv')
        features_with_labels.to_csv(features_with_labels_path, index=False)

        # èšç±»ç»Ÿè®¡
        cluster_stats_path = os.path.join(clustering_dir, 'cluster_statistics.csv')
        cluster_stats_df.to_csv(cluster_stats_path, index=False)

        # èšç±»é…ç½®æ–‡ä»¶
        cluster_profiles_path = os.path.join(clustering_dir, 'cluster_profiles.json')
        with open(cluster_profiles_path, 'w', encoding='utf-8') as f:
            # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
            def convert_for_json(obj):
                if isinstance(obj, (np.integer, np.int64)):
                    return int(obj)
                elif isinstance(obj, (np.floating, np.float64)):
                    return float(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, dict):
                    return {k: convert_for_json(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_for_json(item) for item in obj]
                else:
                    return obj

            json.dump(convert_for_json(cluster_profiles), f, indent=2, ensure_ascii=False)

        print(f"âœ… èšç±»åˆ†æå®Œæˆ!")
        print(f"  èšç±»æ•°é‡: {n_clusters}")
        print(f"  å¸¦æ ‡ç­¾æ•°æ®: {features_with_labels_path}")
        print(f"  èšç±»ç»Ÿè®¡: {cluster_stats_path}")
        print(f"  èšç±»é…ç½®: {cluster_profiles_path}")

        # æ˜¾ç¤ºèšç±»æ¦‚å†µ
        print(f"\nğŸ“Š èšç±»æ¦‚å†µ:")
        for _, row in cluster_stats_df.iterrows():
            cluster_id = row['cluster_id']
            size = row['size']
            percentage = row['percentage']
            print(f"  èšç±» {cluster_id}: {size} ä¸ªä»»åŠ¡ ({percentage:.1f}%)")

        self.results.update({
            'clustering': clustering,
            'embeddings': embeddings,
            'labels': labels,
            'cluster_stats_df': cluster_stats_df,
            'cluster_profiles': cluster_profiles,
            'features_with_labels': features_with_labels
        })

        return clustering, embeddings, labels

    def run_visualization(self, df, features_df, clustering, embeddings, labels):
        """æ­¥éª¤4: å¯è§†åŒ–"""
        print("\n" + "="*60)
        print("æ­¥éª¤4: å¯è§†åŒ–")
        print("="*60)

        if clustering is None:
            print("âŒ èšç±»ç»“æœä¸ºç©ºï¼Œè·³è¿‡å¯è§†åŒ–")
            return

        # åˆ›å»ºå¯è§†åŒ–è¾“å‡ºç›®å½•
        viz_output_dir = os.path.join(self.output_dir, 'visualizations')
        os.makedirs(viz_output_dir, exist_ok=True)

        # åˆ›å»ºå¯è§†åŒ–ç³»ç»Ÿ
        viz = VisualizationSystem(
            config=VISUALIZATION_CONFIG,
            output_dir=viz_output_dir
        )

        print("åˆ›å»ºå¯è§†åŒ–å›¾è¡¨...")

        try:
            # 1. èšç±»æ•£ç‚¹å›¾
            print("  åˆ›å»ºèšç±»æ•£ç‚¹å›¾...")
            viz.create_cluster_scatter(
                embeddings,
                labels,
                title="HPC Workload Clustering"
            )

            # 2. ç‰¹å¾åˆ†å¸ƒå°æç´å›¾
            print("  åˆ›å»ºç‰¹å¾åˆ†å¸ƒå°æç´å›¾...")
            features_with_labels = features_df.copy()
            features_with_labels['cluster'] = labels[:len(features_df)]
            viz.create_violin_plots(features_with_labels, cluster_col='cluster')

            # 3. èšç±»çƒ­åŠ›å›¾
            print("  åˆ›å»ºèšç±»çƒ­åŠ›å›¾...")
            viz.create_heatmap(features_with_labels, cluster_col='cluster')

            # 4. è´¨å¿ƒå›¾ï¼ˆå¦‚æœæœ‰ï¼‰
            if hasattr(clustering, 'cluster_centers') and clustering.cluster_centers is not None:
                print("  åˆ›å»ºè´¨å¿ƒå›¾...")
                viz.create_centroid_plot(
                    clustering.cluster_centers,
                    clustering.feature_names
                )

            # 5. æ—¶é—´åºåˆ—å åŠ å›¾
            print("  åˆ›å»ºæ—¶é—´åºåˆ—å åŠ å›¾...")
            # å‡†å¤‡ä»»åŠ¡IDåˆ—è¡¨
            job_task_ids = []
            if 'job_id' in features_df.columns and 'task_index' in features_df.columns:
                # ç›´æ¥ä»ç‰¹å¾DataFrameè·å–ä»»åŠ¡ID
                for _, row in features_df.iterrows():
                    job_task_ids.append((row['job_id'], row['task_index']))
            else:
                # ä»åŸå§‹æ•°æ®è·å–
                task_groups = df.groupby(['job_id', 'task_index'])
                task_keys = list(task_groups.groups.keys())

                # ç¡®ä¿ä»»åŠ¡IDä¸ç‰¹å¾é¡ºåºå¯¹åº”
                for i in range(min(len(features_df), len(task_keys))):
                    job_id, task_index = task_keys[i]
                    job_task_ids.append((job_id, task_index))

            if job_task_ids:
                viz.create_time_series_overlay(
                    df, labels, job_task_ids, n_samples=3
                )
            else:
                print("  âš ï¸ æ— æ³•åˆ›å»ºæ—¶é—´åºåˆ—å›¾ï¼šç¼ºå°‘ä»»åŠ¡IDä¿¡æ¯")

            # 6. èšç±»ä»ªè¡¨æ¿
            print("  åˆ›å»ºèšç±»ä»ªè¡¨æ¿...")
            if 'cluster_stats_df' in self.results and 'cluster_profiles' in self.results:
                viz.create_dashboard(
                    self.results['cluster_stats_df'],
                    self.results['cluster_profiles']
                )
            else:
                print("  âš ï¸ æ— æ³•åˆ›å»ºä»ªè¡¨æ¿ï¼šç¼ºå°‘èšç±»ç»Ÿè®¡ä¿¡æ¯")

            # ä¿å­˜æ‰€æœ‰å›¾å½¢
            viz.save_all_figures()

            print(f"âœ… å¯è§†åŒ–å®Œæˆ!")
            print(f"  æ‰€æœ‰å›¾è¡¨å·²ä¿å­˜åˆ°: {viz_output_dir}")

            self.results['visualization'] = viz

        except Exception as e:
            print(f"âš ï¸ å¯è§†åŒ–åˆ›å»ºè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()

    def generate_report(self):
        """æ­¥éª¤5: ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print("\n" + "="*60)
        print("æ­¥éª¤5: ç”Ÿæˆåˆ†ææŠ¥å‘Š")
        print("="*60)

        reports_dir = os.path.join(self.output_dir, 'reports')

        # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        report_path = os.path.join(reports_dir, 'analysis_report.txt')

        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("="*60 + "\n")
            f.write("HPCå·¥ä½œè´Ÿè½½åˆ†æä¸åˆ†ç±»æŠ¥å‘Š\n")
            f.write("="*60 + "\n\n")

            f.write(f"åˆ†ææ—¶é—´: {self.timestamp}\n")
            f.write(f"è¾“å‡ºç›®å½•: {self.output_dir}\n\n")

            # 1. æ•°æ®æ¦‚å†µ
            f.write("1. æ•°æ®æ¦‚å†µ\n")
            f.write("-"*40 + "\n")
            if 'raw_data' in self.results:
                df = self.results['raw_data']
                f.write(f"   æ€»æ•°æ®ç‚¹: {len(df):,}\n")
                f.write(f"   å”¯ä¸€ä»»åŠ¡æ•°: {df.groupby(['job_id', 'task_index']).ngroups}\n")
                f.write(f"   æ—¶é—´èŒƒå›´: {df['start_time'].min()} - {df['start_time'].max()}\n\n")

            # 2. ç‰¹å¾å·¥ç¨‹
            f.write("2. ç‰¹å¾å·¥ç¨‹\n")
            f.write("-"*40 + "\n")
            if 'features_df' in self.results:
                features_df = self.results['features_df']
                f.write(f"   ç‰¹å¾æ•°é‡: {features_df.shape[1]}\n")
                f.write(f"   ä»»åŠ¡æ•°é‡: {features_df.shape[0]}\n")

                if 'optimized_params' in self.results:
                    params = self.results['optimized_params']
                    f.write(f"   ä¼˜åŒ–å‚æ•°: {params}\n")
                    f.write(f"   ä¼˜åŒ–å¾—åˆ†: {self.results.get('optimization_score', 'N/A')}\n")
                f.write("\n")

            # 3. èšç±»ç»“æœ
            f.write("3. èšç±»ç»“æœ\n")
            f.write("-"*40 + "\n")
            if 'cluster_stats_df' in self.results:
                cluster_stats_df = self.results['cluster_stats_df']
                f.write(f"   èšç±»æ•°é‡: {len(cluster_stats_df)}\n\n")

                f.write("   èšç±»åˆ†å¸ƒ:\n")
                for _, row in cluster_stats_df.iterrows():
                    cluster_id = row['cluster_id']
                    size = row['size']
                    percentage = row['percentage']
                    f.write(f"     èšç±» {cluster_id}: {size} ä¸ªä»»åŠ¡ ({percentage:.1f}%)\n")
                f.write("\n")

            # 4. å·¥ä½œè´Ÿè½½ç±»å‹æè¿°
            f.write("4. å·¥ä½œè´Ÿè½½ç±»å‹æè¿°\n")
            f.write("-"*40 + "\n")
            if 'cluster_profiles' in self.results:
                cluster_profiles = self.results['cluster_profiles']

                for cluster_id, profile in cluster_profiles.items():
                    f.write(f"\n   èšç±» {cluster_id}:\n")
                    f.write(f"     èµ„æºå¼ºåº¦: {profile.get('resource_intensity', 'N/A')}\n")
                    f.write(f"     ä¸»å¯¼èµ„æº: {profile.get('dominant_resource', 'N/A')}\n")
                    f.write(f"     è¡Œä¸ºæ¨¡å¼: {profile.get('behavior_type', 'N/A')}\n")
                    f.write(f"     æ³¢åŠ¨ç‰¹æ€§: {profile.get('volatility_level', 'N/A')}\n")

                    # è°ƒåº¦å»ºè®®
                    f.write(f"     è°ƒåº¦å»ºè®®: ")
                    resource = profile.get('dominant_resource', '')
                    intensity = profile.get('resource_intensity', '')
                    volatility = profile.get('volatility_level', '')

                    if intensity == 'High' and resource:
                        if resource == 'CPU':
                            f.write("åˆ†é…é«˜CPUèŠ‚ç‚¹ï¼Œè€ƒè™‘CPUäº²å’Œæ€§\n")
                        elif resource == 'Memory':
                            f.write("ä¿è¯è¶³å¤Ÿå†…å­˜ï¼Œé¿å…swap\n")
                        elif resource == 'IO':
                            f.write("ä½¿ç”¨é«˜é€Ÿå­˜å‚¨ï¼Œä¼˜åŒ–IOè°ƒåº¦\n")
                        else:
                            f.write("æ ¹æ®ä¸»å¯¼èµ„æºè¿›è¡Œä¸“é¡¹ä¼˜åŒ–\n")
                    elif volatility == 'High':
                        f.write("é¢„ç•™ç¼“å†²èµ„æºï¼Œä½¿ç”¨å¼¹æ€§è°ƒåº¦ç­–ç•¥\n")
                    else:
                        f.write("æ ‡å‡†è°ƒåº¦ç­–ç•¥ï¼Œèµ„æºæŒ‰éœ€åˆ†é…\n")
                f.write("\n")

            # 5. åº”ç”¨å»ºè®®
            f.write("5. åº”ç”¨å»ºè®®\n")
            f.write("-"*40 + "\n")
            f.write("   â€¢ è°ƒåº¦ä¼˜åŒ–: åŸºäºå·¥ä½œè´Ÿè½½ç±»å‹å®ç°å·®å¼‚åŒ–è°ƒåº¦\n")
            f.write("   â€¢ èµ„æºåˆ†é…: ä¸ºä¸åŒç±»åˆ«è®¾ç½®èµ„æºä¿éšœå’Œé™åˆ¶\n")
            f.write("   â€¢ å®¹é‡è§„åˆ’: è¯†åˆ«é›†ç¾¤èµ„æºç“¶é¢ˆï¼Œä¼˜åŒ–èµ„æºé…ç½®\n")
            f.write("   â€¢ æ€§èƒ½é¢„æµ‹: å»ºç«‹åŸºäºç±»å‹çš„æ€§èƒ½é¢„æµ‹æ¨¡å‹\n")
            f.write("   â€¢ ä»¿çœŸç ”ç©¶: ä½¿ç”¨èšç±»ç»“æœä½œä¸ºGAN/TimeGANè¾“å…¥\n")
            f.write("   â€¢ èµ„æºç«äº‰åˆ†æ: åˆ†æä¸åŒç±»å‹ä»»åŠ¡é—´çš„èµ„æºç«äº‰æ¨¡å¼\n")
            f.write("   â€¢ è°ƒåº¦ç­–ç•¥ä¼˜åŒ–: ä¸ºä¸åŒç±»å‹ä»»åŠ¡åˆ¶å®šæœ€ä¼˜è°ƒåº¦ç­–ç•¥\n")
            f.write("\n")

            # 6. æ–‡ä»¶æ¸…å•
            f.write("6. ç”Ÿæˆæ–‡ä»¶æ¸…å•\n")
            f.write("-"*40 + "\n")

            def list_files(dir_path, indent=4):
                for root, dirs, files in os.walk(dir_path):
                    level = root.replace(dir_path, '').count(os.sep)
                    indent_str = ' ' * indent * level
                    f.write(f"{indent_str}{os.path.basename(root)}/\n")

                    subindent = ' ' * indent * (level + 1)
                    for file in files:
                        f.write(f"{subindent}{file}\n")

            f.write(f"\n{self.output_dir}/\n")
            list_files(self.output_dir, indent=4)

        print(f"âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ!")
        print(f"  æŠ¥å‘Šæ–‡ä»¶: {report_path}")

        # åœ¨æ§åˆ¶å°ä¹Ÿæ˜¾ç¤ºæŠ¥å‘Šæ‘˜è¦
        print("\n" + "="*60)
        print("æŠ¥å‘Šæ‘˜è¦")
        print("="*60)

        if 'cluster_profiles' in self.results:
            print("\nå‘ç°çš„å·¥ä½œè´Ÿè½½ç±»å‹:")
            for cluster_id, profile in self.results['cluster_profiles'].items():
                print(f"\n  ç±»å‹ {cluster_id}:")
                print(f"    ç‰¹å¾: {profile.get('resource_intensity', '')}-{profile.get('dominant_resource', '')}")
                print(f"    è¡Œä¸º: {profile.get('behavior_type', '')}, {profile.get('volatility_level', '')}æ³¢åŠ¨")

        print(f"\nè¯¦ç»†æŠ¥å‘Šè¯·æŸ¥çœ‹: {report_path}")
        print(f"æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {self.output_dir}")

    def save_final_results(self):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        print("\nä¿å­˜æœ€ç»ˆç»“æœ...")

        # ä¿å­˜pickleæ–‡ä»¶
        pickle_path = os.path.join(self.output_dir, 'final_results.pkl')
        with open(pickle_path, 'wb') as f:
            # ç§»é™¤å¯èƒ½æ— æ³•pickleçš„å¯¹è±¡
            save_results = self.results.copy()
            if 'visualization' in save_results:
                del save_results['visualization']
            if 'raw_data' in save_results and isinstance(save_results['raw_data'], pd.DataFrame):
                # åªä¿å­˜æ•°æ®è·¯å¾„ï¼Œä¸ä¿å­˜æ•´ä¸ªDataFrame
                save_results['raw_data'] = None

            pickle.dump(save_results, f)

        print(f"âœ… æœ€ç»ˆç»“æœå·²ä¿å­˜: {pickle_path}")

    def run_complete_analysis(self, sample_size=None, n_clusters=None, optimize_params=None):
        """è¿è¡Œå®Œæ•´åˆ†ææµç¨‹"""
        print("="*80)
        print("HPCå·¥ä½œè´Ÿè½½åˆ†æä¸åˆ†ç±»ç³»ç»Ÿ")
        print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*80)

        # åˆ›å»ºç›®å½•ç»“æ„
        self.setup_directories()

        # æ­¥éª¤1: æ•°æ®åŠ è½½
        df = self.run_data_loading(sample_size)
        if df is None:
            return None

        # æ­¥éª¤2: ç‰¹å¾å·¥ç¨‹
        optimize = optimize_params if optimize_params is not None else self.config.get('optimize_params', True)
        features_df = self.run_feature_engineering(df, optimize_params=optimize)
        if features_df is None:
            return None

        # æ­¥éª¤3: èšç±»åˆ†æ
        n_clusters = n_clusters or self.config.get('n_clusters', 5)
        clustering_result = self.run_clustering(features_df, n_clusters=n_clusters)
        if clustering_result is None:
            return None

        clustering, embeddings, labels = clustering_result

        # æ­¥éª¤4: å¯è§†åŒ–
        self.run_visualization(df, features_df, clustering, embeddings, labels)

        # æ­¥éª¤5: ç”ŸæˆæŠ¥å‘Š
        self.generate_report()

        # ä¿å­˜æœ€ç»ˆç»“æœ
        self.save_final_results()

        print("\n" + "="*80)
        print("âœ… åˆ†æå®Œæˆ!")
        print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {self.output_dir}")
        print("="*80)

        return self.results

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='HPCå·¥ä½œè´Ÿè½½åˆ†æä¸åˆ†ç±»ç³»ç»Ÿ')
    parser.add_argument('--sample_size', type=int, default=None,
                        help='æ ·æœ¬å¤§å°ï¼ˆé»˜è®¤: ä½¿ç”¨é…ç½®æ–‡ä»¶è®¾ç½®ï¼‰')
    parser.add_argument('--max_tasks', type=int, default=None,
                        help='æœ€å¤§ä»»åŠ¡æ•°ï¼ˆé»˜è®¤: ä½¿ç”¨é…ç½®æ–‡ä»¶è®¾ç½®ï¼‰')
    parser.add_argument('--n_clusters', type=int, default=None,
                        help='èšç±»æ•°é‡ï¼ˆé»˜è®¤: 5ï¼‰')
    parser.add_argument('--optimize', action='store_true', default=None,
                        help='å¯ç”¨å‚æ•°ä¼˜åŒ–')
    parser.add_argument('--no_optimize', dest='optimize', action='store_false',
                        help='ç¦ç”¨å‚æ•°ä¼˜åŒ–')
    parser.add_argument('--output_dir', type=str, default=None,
                        help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: è‡ªåŠ¨ç”Ÿæˆæ—¶é—´æˆ³ç›®å½•ï¼‰')

    args = parser.parse_args()

    # æ›´æ–°é…ç½®
    config = EXPERIMENT_CONFIG.copy()

    if args.sample_size is not None:
        config['sample_size'] = args.sample_size

    if args.max_tasks is not None:
        config['max_tasks'] = args.max_tasks

    if args.optimize is not None:
        config['optimize_params'] = args.optimize

    # åˆ›å»ºåˆ†æå™¨
    analyzer = HPCWorkloadAnalyzer(config)

    # è¿è¡Œå®Œæ•´åˆ†æ
    results = analyzer.run_complete_analysis(
        sample_size=args.sample_size,
        n_clusters=args.n_clusters,
        optimize_params=args.optimize
    )

    return results

if __name__ == "__main__":
    # è¿è¡Œå®Œæ•´åˆ†æ
    main()