"""
ä¿®å¤ç‰ˆæ•°æ®åº“æ¨¡å— - å¤„ç†æ•°æ®ç±»å‹é—®é¢˜
"""
import pandas as pd
import mysql.connector
from mysql.connector import Error
import numpy as np
from config import DB_CONFIG

class DatabaseConnector:
    def __init__(self, config=None):
        self.config = config or DB_CONFIG
        self.connection = None

    def connect(self):
        """å»ºç«‹æ•°æ®åº“è¿æ¥"""
        try:
            self.connection = mysql.connector.connect(**self.config)
            if self.connection.is_connected():
                print(f"æˆåŠŸè¿æ¥åˆ°æ•°æ®åº“ {self.config['database']}")
                return True
        except Error as e:
            print(f"æ•°æ®åº“è¿æ¥é”™è¯¯: {e}")
            return False

    def disconnect(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.connection and self.connection.is_connected():
            self.connection.close()
            print("æ•°æ®åº“è¿æ¥å·²å…³é—­")

    def _convert_to_native_types(self, params):
        """å°†numpyç±»å‹è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹"""
        if params is None:
            return None

        if isinstance(params, (list, tuple)):
            return [self._convert_to_native_types(p) for p in params]
        elif isinstance(params, dict):
            return {k: self._convert_to_native_types(v) for k, v in params.items()}
        elif isinstance(params, np.integer):
            return int(params)
        elif isinstance(params, np.floating):
            return float(params)
        elif isinstance(params, np.ndarray):
            return params.tolist()
        else:
            return params

    def execute_query(self, query, params=None):
        """æ‰§è¡ŒæŸ¥è¯¢å¹¶å¤„ç†æ•°æ®ç±»å‹"""
        try:
            # è½¬æ¢å‚æ•°ç±»å‹
            native_params = self._convert_to_native_types(params)

            # ä½¿ç”¨pandasçš„read_sqlï¼Œå®ƒä¼šè‡ªåŠ¨å¤„ç†ç±»å‹è½¬æ¢
            df = pd.read_sql(query, self.connection, params=native_params)
            return df
        except Exception as e:
            print(f"æŸ¥è¯¢æ‰§è¡Œé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return None

    def load_task_time_series(self, num_tasks=100, min_samples=5):
        """
        åŠ è½½ä»»åŠ¡çš„å®Œæ•´æ—¶é—´åºåˆ—æ•°æ®

        Args:
            num_tasks: è¦åŠ è½½çš„ä»»åŠ¡æ•°é‡
            min_samples: æ¯ä¸ªä»»åŠ¡æœ€å°‘éœ€è¦çš„æ•°æ®ç‚¹æ•°
        """
        print(f"åŠ è½½ {num_tasks} ä¸ªä»»åŠ¡çš„å®Œæ•´æ—¶é—´åºåˆ—æ•°æ®...")

        try:
            # æ­¥éª¤1: å…ˆæ‰¾åˆ°æœ‰è¶³å¤Ÿæ•°æ®ç‚¹çš„ä»»åŠ¡
            print("æŸ¥æ‰¾æœ‰è¶³å¤Ÿæ•°æ®ç‚¹çš„ä»»åŠ¡...")

            # æŸ¥è¯¢æ¯ä¸ªä»»åŠ¡çš„æ•°æ®ç‚¹æ•°é‡
            task_count_query = """
                               SELECT
                                   job_id, task_index, COUNT(*) as num_samples,
                                   MIN(start_time) as first_time,
                                   MAX(end_time) as last_time
                               FROM task_usage
                               WHERE cpu_rate IS NOT NULL
                                 AND canonical_memory_usage IS NOT NULL
                               GROUP BY job_id, task_index
                               HAVING COUNT(*) >= %s
                               ORDER BY num_samples DESC
                                   LIMIT %s \
                               """

            # ä½¿ç”¨execute_queryæ¥å¤„ç†æ•°æ®ç±»å‹
            task_counts = self.execute_query(task_count_query,
                                             params=[min_samples, num_tasks * 2])

            if task_counts is None or len(task_counts) == 0:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰è¶³å¤Ÿæ•°æ®ç‚¹çš„ä»»åŠ¡")
                return None

            print(f"æ‰¾åˆ° {len(task_counts)} ä¸ªæœ‰è¶³å¤Ÿæ•°æ®ç‚¹çš„ä»»åŠ¡")

            # æ­¥éª¤2: æ‰¹é‡åŠ è½½ä»»åŠ¡æ•°æ®ï¼ˆæ›´é«˜æ•ˆçš„æ–¹å¼ï¼‰
            print("æ‰¹é‡åŠ è½½ä»»åŠ¡æ•°æ®...")

            # æ”¶é›†æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„ä»»åŠ¡ID
            task_ids = []
            for _, row in task_counts.iterrows():
                # è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹
                job_id = int(row['job_id'])
                task_index = int(row['task_index'])
                task_ids.append((job_id, task_index))

            # æ„å»ºINæŸ¥è¯¢ï¼ˆæ›´é«˜æ•ˆï¼‰
            if not task_ids:
                print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„ä»»åŠ¡ID")
                return None

            # é™åˆ¶ä»»åŠ¡æ•°é‡
            task_ids = task_ids[:num_tasks]

            # æ„å»ºWHEREæ¡ä»¶
            conditions = []
            params = []
            for job_id, task_index in task_ids:
                conditions.append("(job_id = %s AND task_index = %s)")
                params.extend([job_id, task_index])

            where_clause = " OR ".join(conditions)

            # ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰ä»»åŠ¡æ•°æ®
            batch_query = f"""
            SELECT 
                start_time, end_time, job_id, task_index, machine_id,
                cpu_rate, canonical_memory_usage, disk_io_time,
                maximum_cpu_rate, maximum_memory_usage,
                local_disk_space_usage, total_page_cache,
                cycles_per_instruction
            FROM task_usage
            WHERE ({where_clause})
            ORDER BY job_id, task_index, start_time
            """

            df = self.execute_query(batch_query, params=params)

            if df is None or len(df) == 0:
                print("âŒ æ‰¹é‡åŠ è½½æ•°æ®å¤±è´¥")
                return None

            print(f"æ‰¹é‡åŠ è½½å®Œæˆï¼Œå…± {len(df)} æ¡è®°å½•")

            # æ£€æŸ¥æ•°æ®è´¨é‡
            task_groups = df.groupby(['job_id', 'task_index'])
            valid_tasks = []

            for (job_id, task_index), group in task_groups:
                if len(group) >= min_samples:
                    # ç¡®ä¿æ•°æ®æŒ‰æ—¶é—´æ’åº
                    group = group.sort_values('start_time').copy()

                    # è®¡ç®—è¡ç”Ÿç‰¹å¾
                    group['duration'] = group['end_time'] - group['start_time']
                    group['time_from_start'] = group['start_time'] - group['start_time'].min()

                    valid_tasks.append(group)

            if not valid_tasks:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰è¶³å¤Ÿæ•°æ®ç‚¹çš„æœ‰æ•ˆä»»åŠ¡")
                return None

            # åˆå¹¶æ‰€æœ‰æœ‰æ•ˆä»»åŠ¡æ•°æ®
            combined_df = pd.concat(valid_tasks, ignore_index=True)

            print(f"\nâœ… æˆåŠŸåŠ è½½ {len(valid_tasks)} ä¸ªä»»åŠ¡çš„å®Œæ•´æ—¶é—´åºåˆ—")
            print(f"æ€»æ•°æ®ç‚¹: {len(combined_df)}")
            print(f"æ¯ä¸ªä»»åŠ¡å¹³å‡æ•°æ®ç‚¹: {len(combined_df) / len(valid_tasks):.1f}")

            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            task_stats = []
            for (job_id, task_index), group in combined_df.groupby(['job_id', 'task_index']):
                task_stats.append({
                    'job_id': job_id,
                    'task_index': task_index,
                    'num_samples': len(group),
                    'time_span': group['end_time'].max() - group['start_time'].min(),
                    'cpu_mean': group['cpu_rate'].mean(),
                    'cpu_std': group['cpu_rate'].std(),
                    'mem_mean': group['canonical_memory_usage'].mean(),
                    'mem_std': group['canonical_memory_usage'].std()
                })

            stats_df = pd.DataFrame(task_stats)

            print(f"\nğŸ“Š ä»»åŠ¡ç»Ÿè®¡:")
            print(f"  ä»»åŠ¡æ•°é‡: {len(stats_df)}")
            print(f"  æœ€å°åºåˆ—é•¿åº¦: {stats_df['num_samples'].min()}")
            print(f"  æœ€å¤§åºåˆ—é•¿åº¦: {stats_df['num_samples'].max()}")
            print(f"  å¹³å‡åºåˆ—é•¿åº¦: {stats_df['num_samples'].mean():.1f}")
            print(f"  æœ€å°æ—¶é—´è·¨åº¦: {stats_df['time_span'].min()}")
            print(f"  æœ€å¤§æ—¶é—´è·¨åº¦: {stats_df['time_span'].max()}")
            print(f"  å¹³å‡CPUä½¿ç”¨ç‡: {stats_df['cpu_mean'].mean():.4f}")
            print(f"  å¹³å‡å†…å­˜ä½¿ç”¨: {stats_df['mem_mean'].mean():.1f}")

            return combined_df

        except Exception as e:
            print(f"æ•°æ®åŠ è½½é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return None

    def load_high_frequency_tasks(self, num_tasks=50, frequency_threshold=30):
        """
        åŠ è½½é«˜é¢‘ä»»åŠ¡ï¼ˆæœ‰å¤§é‡æ•°æ®ç‚¹çš„ä»»åŠ¡ï¼‰
        """
        print(f"åŠ è½½é«˜é¢‘ä»»åŠ¡æ•°æ®ï¼ˆè‡³å°‘ {frequency_threshold} ä¸ªæ•°æ®ç‚¹ï¼‰...")

        try:
            # æŸ¥è¯¢é«˜é¢‘ä»»åŠ¡
            query = """
                    SELECT
                        job_id, task_index, COUNT(*) as num_samples,
                        AVG(cpu_rate) as avg_cpu,
                        AVG(canonical_memory_usage) as avg_mem
                    FROM task_usage
                    WHERE cpu_rate IS NOT NULL
                      AND canonical_memory_usage IS NOT NULL
                    GROUP BY job_id, task_index
                    HAVING COUNT(*) >= %s
                    ORDER BY num_samples DESC
                        LIMIT %s \
                    """

            tasks = self.execute_query(query,
                                       params=[frequency_threshold, num_tasks])

            if tasks is None or len(tasks) == 0:
                print(f"âŒ æ²¡æœ‰æ‰¾åˆ°è‡³å°‘æœ‰ {frequency_threshold} ä¸ªæ•°æ®ç‚¹çš„ä»»åŠ¡")
                return None

            print(f"æ‰¾åˆ° {len(tasks)} ä¸ªé«˜é¢‘ä»»åŠ¡")

            # æ‰¹é‡åŠ è½½è¿™äº›ä»»åŠ¡çš„æ•°æ®
            task_ids = []
            for _, row in tasks.iterrows():
                task_ids.append((int(row['job_id']), int(row['task_index'])))

            # æ„å»ºæŸ¥è¯¢
            conditions = []
            params = []
            for job_id, task_index in task_ids:
                conditions.append("(job_id = %s AND task_index = %s)")
                params.extend([job_id, task_index])

            where_clause = " OR ".join(conditions)

            batch_query = f"""
            SELECT 
                start_time, end_time, job_id, task_index, machine_id,
                cpu_rate, canonical_memory_usage, disk_io_time,
                maximum_cpu_rate, maximum_memory_usage,
                local_disk_space_usage, total_page_cache,
                cycles_per_instruction
            FROM task_usage
            WHERE ({where_clause})
            ORDER BY job_id, task_index, start_time
            """

            df = self.execute_query(batch_query, params=params)

            if df is None or len(df) == 0:
                return None

            # å¤„ç†æ•°æ®
            df = df.sort_values(['job_id', 'task_index', 'start_time']).copy()
            df['duration'] = df['end_time'] - df['start_time']

            print(f"âœ… åŠ è½½äº† {len(df)} æ¡é«˜é¢‘ä»»åŠ¡è®°å½•")

            return df

        except Exception as e:
            print(f"é«˜é¢‘ä»»åŠ¡åŠ è½½é”™è¯¯: {e}")
            return None